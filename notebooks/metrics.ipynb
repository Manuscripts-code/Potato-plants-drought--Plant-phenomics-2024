{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "from operator import itemgetter\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from rich.progress import track\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from notebooks.helpers import import_artifacts_from_runID\n",
    "from utils.utils import ensure_dir, write_txt\n",
    "from utils.tools import calculate_metric_and_confidence_interval\n",
    "from utils.plot_utils import plot_roc_curves\n",
    "from configs import configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_df(run_id):\n",
    "\tartifacts = import_artifacts_from_runID(run_id)\n",
    "\tmodel, data_loader, device, config = itemgetter(\"model\", \"data_loader\", \"device\", \"config\")(\n",
    "\t\tartifacts\n",
    "\t)\n",
    "\n",
    "\tpredictions_test = []\n",
    "\ttargets_test = []\n",
    "\tlabels_test = []\n",
    "\timagings_test = []\n",
    "\twith torch.no_grad():\n",
    "\t\tfor data, target, metadata in track(data_loader, description=\"Loading data...\"):\n",
    "\t\t\tprediction = model(data.to(device))\n",
    "\n",
    "\t\t\tpredictions_test.append(prediction.cpu().numpy())\n",
    "\t\t\ttargets_test.append(target.numpy())\n",
    "\t\t\tlabels_test.append(metadata[\"label\"])\n",
    "\t\t\timagings_test.append(metadata[\"imaging\"])\n",
    "\t\n",
    "\tclear_output(wait=True)\n",
    "\n",
    "\tpredictions_test = np.concatenate(predictions_test).flatten()\n",
    "\ttargets_test = np.concatenate(targets_test).flatten()\n",
    "\tlabels_test = np.concatenate(labels_test).flatten()\n",
    "\timagings_test = np.concatenate(imagings_test).flatten()\n",
    "\n",
    "\ttest_df = pd.DataFrame.from_dict(\n",
    "\t\t{\n",
    "\t\t\t\"imaging\": imagings_test,\n",
    "\t\t\t\"label\": labels_test,\n",
    "\t\t\t\"target\": targets_test,\n",
    "\t\t\t\"prediction\": predictions_test.round().astype(\"int\"),\n",
    "\t\t\t\"prediction_proba\": predictions_test,\n",
    "\t\t}\n",
    "\t)\n",
    "\treturn test_df, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_per_imaging_report(test_df, add_counts=False):\n",
    "\tmsg = \"\"\n",
    "\tST_SPACE = 20\n",
    "\tfor name, df in test_df.groupby(\"imaging\"):\n",
    "\t\tmsg += f\"{name:<{ST_SPACE}}\"\n",
    "\t\tmean, ci = calculate_metric_and_confidence_interval(df, roc_auc_score, \"prediction_proba\")\n",
    "\t\tmsg += f\"{mean:.3f} ({ci[0]:.3f}, {ci[1]:.3f})    \"\n",
    "\t\tmean, ci = calculate_metric_and_confidence_interval(df, f1_score)\n",
    "\t\tmsg += f\"{mean:.3f} ({ci[0]:.3f}, {ci[1]:.3f})    \"\n",
    "\t\tmean, ci = calculate_metric_and_confidence_interval(df, precision_score)\n",
    "\t\tmsg += f\"{mean:.3f} ({ci[0]:.3f}, {ci[1]:.3f})   \"\n",
    "\t\tmean, ci = calculate_metric_and_confidence_interval(df, recall_score)\n",
    "\t\tmsg += f\"{mean:.3f} ({ci[0]:.3f}, {ci[1]:.3f})   \\n\"\n",
    "\tmsg += f\"{'Total':<{ST_SPACE}}\" \n",
    "\tmean, ci = calculate_metric_and_confidence_interval(test_df, roc_auc_score, \"prediction_proba\")\n",
    "\tmsg += f\"{mean:.3f} ({ci[0]:.3f}, {ci[1]:.3f})    \"\n",
    "\tmean, ci = calculate_metric_and_confidence_interval(test_df, f1_score)\n",
    "\tmsg += f\"{mean:.3f} ({ci[0]:.3f}, {ci[1]:.3f})    \"\n",
    "\tmean, ci = calculate_metric_and_confidence_interval(test_df, precision_score)\n",
    "\tmsg += f\"{mean:.3f} ({ci[0]:.3f}, {ci[1]:.3f})   \"\n",
    "\tmean, ci = calculate_metric_and_confidence_interval(test_df, recall_score)\n",
    "\tmsg += f\"{mean:.3f} ({ci[0]:.3f}, {ci[1]:.3f})   \\n\\n\"\n",
    "\tif add_counts:\n",
    "\t\tmsg += test_df.astype(\"object\").groupby(\"imaging\").count().to_string()\n",
    "\treturn msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get experiments ids-es and remove .trash\n",
    "experiments_ids = [f for f in os.listdir(configs.MODEL_REGISTRY) if not f.startswith('.')]\n",
    "# write data dict where key represent experiment by name and value correspond to runs under that experiment\n",
    "data_all = {mlflow.get_experiment(exp_id).name: mlflow.search_runs(exp_id) for exp_id in experiments_ids}\n",
    "# extract test ids\n",
    "run_ids_test = data_all[\"train_CNN\"][\"run_id\"].tolist()\n",
    "# load test dfs\n",
    "test_data = [(load_test_df(run_id)) for run_id in run_ids_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_roc_curves_dir = ensure_dir(configs.BASE_DIR / \"saved/roc_curves\")\n",
    "save_metrics_report_dir = ensure_dir(configs.BASE_DIR / \"saved/metrics_report\")\n",
    "report = \"\"\n",
    "\n",
    "for test_df, config in test_data:\n",
    "\timagings_str = \"\".join(config[\"data_loader\"][\"args\"][\"imagings_used\"])\n",
    "\timagings_str = \"\".join(list(filter(str.isdigit, imagings_str)))\n",
    "\tsampler_str = config[\"data_loader\"][\"args\"][\"data_sampler\"]\n",
    "\tname = sampler_str + imagings_str\n",
    "\n",
    "\tdfs = [df for _, df in test_df.groupby(\"imaging\")]\n",
    "\tplot_roc_curves(dfs, title=name)\n",
    "\tsave_path = save_roc_curves_dir / f\"{name}.pdf\"\n",
    "\tplt.savefig(save_path, format=\"pdf\", bbox_inches=\"tight\")\n",
    "\t\n",
    "\treport += f\"{name}\\n\"\n",
    "\treport += create_per_imaging_report(test_df, add_counts=False)\n",
    "\n",
    "write_txt(report, save_metrics_report_dir / \"metrics_report.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0c53fec6fca4cac05b1ddd19140a9df2d78436a9f74fcfbfef81682f6f651611"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
